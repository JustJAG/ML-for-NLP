{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb568f6-fabe-4832-bb9a-adabbf6e7a31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Challenge 1: Are the top ten words mentioned by Biden in the 105 congress (after stopwords removal). Find the 10 most simlar words generated using word2vec. Is the most frequent bigram included in the the list of most similar word?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0337e-b309-4b8b-9c5b-577e54bd2869",
   "metadata": {},
   "source": [
    "## First, read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c552bbd-cc0e-4fe1-9eb6-5f4529b9fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "## THIS CELL IS FOR PLAYING SOUND AT THE END OF CODE COMPLETION ##\n",
    "##################################################################\n",
    "## Import up sound alert dependencies\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "## Insert whatever audio file you want above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd5c21d-a3ab-487b-88bc-ffc280703b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, LancasterStemmer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356b4aa0-6dc9-4b5b-a361-e21ba7ed5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../Inputs/105-extracted-date/105-biden-de.txt'\n",
    "xml = open(file).read()\n",
    "soup = BeautifulSoup(xml) #use BeautifulSoup library for parsing contents since documents have XML format\n",
    "doc = ''\n",
    "for word in soup.find_all('text'): #loop for extracting only text within <text> tag\n",
    "    find_all_syntax=word.get_text()\n",
    "    doc = doc + find_all_syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab40993-7f3a-4c0c-8c13-8afa21e78e1c",
   "metadata": {},
   "source": [
    "As we read the file, now we need to split it by sentences. Splitting by `\\n` will do the job as the file constructed this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c293de-e5ad-444c-b68a-ca4616525b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. BIDEN. Mr. President, I am pleased that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What this legislation does is simple. Under cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This legislation simply clarifies that the ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr. President, the author of this idea was Rep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                                   \n",
       "1  Mr. BIDEN. Mr. President, I am pleased that th...\n",
       "2  What this legislation does is simple. Under cu...\n",
       "3  This legislation simply clarifies that the ful...\n",
       "4  Mr. President, the author of this idea was Rep..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_split = doc.split('\\n')\n",
    "df = pd.DataFrame(doc_split)\n",
    "df.rename(columns={0: \"text\"},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8283252d-ae92-4b6d-a53c-b39307c2c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of empty rows\n",
    "df = df[df['text']!=\"\"]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92a4fe-f235-4b93-a9a0-9d0240f71589",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b234e1-d6af-447f-8174-67b870fb5693",
   "metadata": {},
   "source": [
    "Now, let's do preprocessing. First, import stopwords, then define the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f734bc3a-3bcc-4bd1-bec5-2b5a2bb210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the stopword list provided by the NLTK library\n",
    "stop_words1 = stopwords.words('english')\n",
    "\n",
    "#the stopword list provided by the professor\n",
    "drop_file = open('../Inputs/droplist.txt', \"r\").read()\n",
    "drop_file = drop_file.replace('\"', '')\n",
    "drop_list = drop_file.split(\"\\n\")\n",
    "stop_words2 = drop_list[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395ad625-6731-4341-8929-3e8a3b40b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_final = set(stop_words1+stop_words2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799051f-cffa-4aae-a0aa-55c529e975e2",
   "metadata": {},
   "source": [
    "Here we start preprocessing, we create a function that takes text as input an does the following:\n",
    "1. replaces all non-alphanumeric characters with spaces\n",
    "2. replaces all letters to lower case\n",
    "3. removes stopwords\n",
    "4. removes words with digits\n",
    "5. conducts a lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601bb0f1-a9a3-4e6b-b07c-376b8541eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9 ]+', ' ', text) #remove all non‐alphanumeric characters except white space\n",
    "    words = word_tokenize(text.lower())\n",
    "    tokens = [word for word in words if word not in stopwords_final]\n",
    "    tokens = [token for token in tokens if not any(c.isdigit() for c in token)] #remove everything containing digits\n",
    "    tokens = [token for token in tokens if len(token)>=3]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lematized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens_lematized)\n",
    "    return preprocessed_text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c7968-5d01-4c85-bc83-e2eef60c0fb2",
   "metadata": {},
   "source": [
    "Preprocess Biden's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0ccf5d-1b5a-4de1-b25f-36d6ee4d9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_prepr'] = df['text'].apply(lambda x: preprocessing_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d8552-49a2-4ad8-b6f6-9455ab5f201b",
   "metadata": {},
   "source": [
    "### Transform the sentence into list\n",
    "Now, prepare the document for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a0c55e-1f3e-40f3-b56a-c1283d444297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def split(text):\n",
    "    text= re.sub(r'\\d', '', text)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7480ead-0d56-4a00-8eca-b020e8f2f7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text_split'] = df['text_prepr'].apply(lambda x: split(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1053df-dd3d-4066-a24f-0eb6ae3c7d57",
   "metadata": {},
   "source": [
    "### Lemmatize each word, drop short words (less than two characters), drop most common words\n",
    "Here, I won't drop anything. That's why I will input empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bced73-b26c-4afd-b740-68e77804d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_lemma(element):\n",
    "    for word, pos in pos_tag(element.split()):\n",
    "        result_pos=wordnet_pos(pos)\n",
    "        if result_pos != None:\n",
    "            return WordNetLemmatizer().lemmatize(word, result_pos)\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "# original example can be found here https://stackoverflow.com/a/15590384\n",
    "def wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def remove_common_words(list_element, common_words):\n",
    "    tokens = [get_lemma(token) for token in list_element]\n",
    "    tokens = [token for token in tokens if token not in common_words]\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97e0126-b7af-4cb7-9f93-d984956416de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_lem'] = df['text_split'].apply(lambda x: remove_common_words(x,['']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c944803-70f3-4f74-a1b9-028881548823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_prepr</th>\n",
       "      <th>text_split</th>\n",
       "      <th>text_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. BIDEN. Mr. President, I am pleased that th...</td>\n",
       "      <td>president pleased senate passing substitute am...</td>\n",
       "      <td>[president, pleased, senate, passing, substitu...</td>\n",
       "      <td>[president, pleased, senate, passing, substitu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What this legislation does is simple. Under cu...</td>\n",
       "      <td>legislation simple current federal law faith c...</td>\n",
       "      <td>[legislation, simple, current, federal, law, f...</td>\n",
       "      <td>[legislation, simple, current, federal, law, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This legislation simply clarifies that the ful...</td>\n",
       "      <td>legislation simply clarifies faith credit law ...</td>\n",
       "      <td>[legislation, simply, clarifies, faith, credit...</td>\n",
       "      <td>[legislation, simply, clarifies, faith, credit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr. President, the author of this idea was Rep...</td>\n",
       "      <td>president author idea representative rob andre...</td>\n",
       "      <td>[president, author, idea, representative, rob,...</td>\n",
       "      <td>[president, author, idea, representative, rob,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally, I want to thank Senator Hatch for his...</td>\n",
       "      <td>finally thank senator willingness move bill fi...</td>\n",
       "      <td>[finally, thank, senator, willingness, move, b...</td>\n",
       "      <td>[finally, thank, senator, willingness, move, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>No where has the crime policy debate been subj...</td>\n",
       "      <td>crime policy debate subject distortion misunde...</td>\n",
       "      <td>[crime, policy, debate, subject, distortion, m...</td>\n",
       "      <td>[crime, policy, debate, subject, distortion, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5796</th>\n",
       "      <td>To get past all the misunderstanding, we propo...</td>\n",
       "      <td>past misunderstanding propose call prestigious...</td>\n",
       "      <td>[past, misunderstanding, propose, call, presti...</td>\n",
       "      <td>[past, misunderstand, propose, call, prestigio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5797</th>\n",
       "      <td>Let me repeat a challenge I offered last week-...</td>\n",
       "      <td>repeat challenge offered week live result stud...</td>\n",
       "      <td>[repeat, challenge, offered, week, live, resul...</td>\n",
       "      <td>[repeat, challenge, offer, week, live, result,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5798</th>\n",
       "      <td>But, in the meantime, it seems to me that we d...</td>\n",
       "      <td>meantime preventing youth crime drug abuse mom...</td>\n",
       "      <td>[meantime, preventing, youth, crime, drug, abu...</td>\n",
       "      <td>[meantime, prevent, youth, crime, drug, abuse,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>This refers to the commonsense notion that if ...</td>\n",
       "      <td>refers commonsense notion kid street supervise...</td>\n",
       "      <td>[refers, commonsense, notion, kid, street, sup...</td>\n",
       "      <td>[refers, commonsense, notion, kid, street, sup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Mr. BIDEN. Mr. President, I am pleased that th...   \n",
       "1     What this legislation does is simple. Under cu...   \n",
       "2     This legislation simply clarifies that the ful...   \n",
       "3     Mr. President, the author of this idea was Rep...   \n",
       "4     Finally, I want to thank Senator Hatch for his...   \n",
       "...                                                 ...   \n",
       "5795  No where has the crime policy debate been subj...   \n",
       "5796  To get past all the misunderstanding, we propo...   \n",
       "5797  Let me repeat a challenge I offered last week-...   \n",
       "5798  But, in the meantime, it seems to me that we d...   \n",
       "5799  This refers to the commonsense notion that if ...   \n",
       "\n",
       "                                             text_prepr  \\\n",
       "0     president pleased senate passing substitute am...   \n",
       "1     legislation simple current federal law faith c...   \n",
       "2     legislation simply clarifies faith credit law ...   \n",
       "3     president author idea representative rob andre...   \n",
       "4     finally thank senator willingness move bill fi...   \n",
       "...                                                 ...   \n",
       "5795  crime policy debate subject distortion misunde...   \n",
       "5796  past misunderstanding propose call prestigious...   \n",
       "5797  repeat challenge offered week live result stud...   \n",
       "5798  meantime preventing youth crime drug abuse mom...   \n",
       "5799  refers commonsense notion kid street supervise...   \n",
       "\n",
       "                                             text_split  \\\n",
       "0     [president, pleased, senate, passing, substitu...   \n",
       "1     [legislation, simple, current, federal, law, f...   \n",
       "2     [legislation, simply, clarifies, faith, credit...   \n",
       "3     [president, author, idea, representative, rob,...   \n",
       "4     [finally, thank, senator, willingness, move, b...   \n",
       "...                                                 ...   \n",
       "5795  [crime, policy, debate, subject, distortion, m...   \n",
       "5796  [past, misunderstanding, propose, call, presti...   \n",
       "5797  [repeat, challenge, offered, week, live, resul...   \n",
       "5798  [meantime, preventing, youth, crime, drug, abu...   \n",
       "5799  [refers, commonsense, notion, kid, street, sup...   \n",
       "\n",
       "                                               text_lem  \n",
       "0     [president, pleased, senate, passing, substitu...  \n",
       "1     [legislation, simple, current, federal, law, f...  \n",
       "2     [legislation, simply, clarifies, faith, credit...  \n",
       "3     [president, author, idea, representative, rob,...  \n",
       "4     [finally, thank, senator, willingness, move, b...  \n",
       "...                                                 ...  \n",
       "5795  [crime, policy, debate, subject, distortion, m...  \n",
       "5796  [past, misunderstand, propose, call, prestigio...  \n",
       "5797  [repeat, challenge, offer, week, live, result,...  \n",
       "5798  [meantime, prevent, youth, crime, drug, abuse,...  \n",
       "5799  [refers, commonsense, notion, kid, street, sup...  \n",
       "\n",
       "[5800 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b0624-d42d-440f-b305-059c560f07b3",
   "metadata": {},
   "source": [
    "## Find all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e157e99-3618-4902-a7c7-4fbc85a619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "c_vec = CountVectorizer(stop_words=stopwords_final, ngram_range=(2,2))\n",
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(df['text_prepr'])\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'ngram'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a06352-0e57-420a-af3a-f1e6db9afb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>164</td>\n",
       "      <td>united nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161</td>\n",
       "      <td>foreign policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155</td>\n",
       "      <td>chemical weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116</td>\n",
       "      <td>nato enlargement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>foreign relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>96</td>\n",
       "      <td>yield floor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96</td>\n",
       "      <td>nuclear weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>87</td>\n",
       "      <td>relation committee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80</td>\n",
       "      <td>north carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75</td>\n",
       "      <td>madam president</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frequency               ngram\n",
       "0        164       united nation\n",
       "1        161      foreign policy\n",
       "2        155     chemical weapon\n",
       "3        116    nato enlargement\n",
       "4        110    foreign relation\n",
       "5         96         yield floor\n",
       "6         96      nuclear weapon\n",
       "7         87  relation committee\n",
       "8         80      north carolina\n",
       "9         75     madam president"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ngram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec8e5fb4-80bb-44ac-95ae-9e74a7dc0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10bigrams = df_ngram.head(10).ngram.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a73032-92d2-48dd-9ae6-d3d82fd2f216",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Find top ten words in Biden's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81abcfdf-04c1-48e7-a0a6-3a9f3001b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec = CountVectorizer(stop_words=stopwords_final, ngram_range=(1,1))\n",
    "# matrix of ngrams\n",
    "ngrams = c_vec.fit_transform(df['text_prepr'])\n",
    "# count frequency of ngrams\n",
    "count_values = ngrams.toarray().sum(axis=0)\n",
    "# list of ngrams\n",
    "vocab = c_vec.vocabulary_\n",
    "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
    "            ).rename(columns={0: 'frequency', 1:'ngram'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab5721c-7de6-4d44-a23a-29901dbbbf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1548</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1104</td>\n",
       "      <td>senator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>985</td>\n",
       "      <td>nato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>653</td>\n",
       "      <td>united</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>580</td>\n",
       "      <td>bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>575</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>531</td>\n",
       "      <td>amendment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>508</td>\n",
       "      <td>nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>478</td>\n",
       "      <td>senate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frequency      ngram\n",
       "0       1548  president\n",
       "1       1104    senator\n",
       "2        985       nato\n",
       "3        700       time\n",
       "4        653     united\n",
       "5        580       bill\n",
       "6        575    country\n",
       "7        531  amendment\n",
       "8        508     nation\n",
       "9        478     senate"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ngram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c56cfdd-777e-46bc-9b88-c0c4589b67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10words = df_ngram.head(10).ngram.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ed55f-3c57-41d3-8a18-86052c42f398",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply Word2Vec to find most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fc44cc8-f251-4fdb-9ff0-e0d160128f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=df.text_lem, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f92384b3-e095-4ac2-b4b6-5d85e04949c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame of similar words\n",
    "df_similar = pd.DataFrame()\n",
    "for i in top10words:\n",
    "    column = []\n",
    "    for j in range(10):\n",
    "        column.append(model.wv.most_similar(i, topn=10)[j][0])\n",
    "    df_similar[i] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "614be9ab-cb35-433e-b711-8dd6bd1c35a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>senator</th>\n",
       "      <th>nato</th>\n",
       "      <th>time</th>\n",
       "      <th>united</th>\n",
       "      <th>bill</th>\n",
       "      <th>country</th>\n",
       "      <th>amendment</th>\n",
       "      <th>nation</th>\n",
       "      <th>senate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>madam</td>\n",
       "      <td>minute</td>\n",
       "      <td>european</td>\n",
       "      <td>friend</td>\n",
       "      <td>nation</td>\n",
       "      <td>introduce</td>\n",
       "      <td>force</td>\n",
       "      <td>speak</td>\n",
       "      <td>united</td>\n",
       "      <td>pleased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy</td>\n",
       "      <td>floor</td>\n",
       "      <td>europe</td>\n",
       "      <td>distinguish</td>\n",
       "      <td>vital</td>\n",
       "      <td>republican</td>\n",
       "      <td>russia</td>\n",
       "      <td>colleague</td>\n",
       "      <td>stability</td>\n",
       "      <td>debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>senior</td>\n",
       "      <td>remainder</td>\n",
       "      <td>russia</td>\n",
       "      <td>colleague</td>\n",
       "      <td>military</td>\n",
       "      <td>offer</td>\n",
       "      <td>power</td>\n",
       "      <td>vote</td>\n",
       "      <td>military</td>\n",
       "      <td>urge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>speak</td>\n",
       "      <td>thank</td>\n",
       "      <td>stability</td>\n",
       "      <td>carolina</td>\n",
       "      <td>protect</td>\n",
       "      <td>issue</td>\n",
       "      <td>military</td>\n",
       "      <td>time</td>\n",
       "      <td>vital</td>\n",
       "      <td>compliment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accommodate</td>\n",
       "      <td>yield</td>\n",
       "      <td>military</td>\n",
       "      <td>chair</td>\n",
       "      <td>stability</td>\n",
       "      <td>legislation</td>\n",
       "      <td>stability</td>\n",
       "      <td>carolina</td>\n",
       "      <td>protect</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vote</td>\n",
       "      <td>distinguish</td>\n",
       "      <td>poland</td>\n",
       "      <td>thank</td>\n",
       "      <td>national</td>\n",
       "      <td>appropriation</td>\n",
       "      <td>threat</td>\n",
       "      <td>president</td>\n",
       "      <td>economic</td>\n",
       "      <td>vote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hampshire</td>\n",
       "      <td>delighted</td>\n",
       "      <td>membership</td>\n",
       "      <td>unanimous</td>\n",
       "      <td>economic</td>\n",
       "      <td>urge</td>\n",
       "      <td>nation</td>\n",
       "      <td>friend</td>\n",
       "      <td>russia</td>\n",
       "      <td>majority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>consent</td>\n",
       "      <td>friend</td>\n",
       "      <td>central</td>\n",
       "      <td>amendment</td>\n",
       "      <td>increase</td>\n",
       "      <td>pass</td>\n",
       "      <td>enter</td>\n",
       "      <td>chair</td>\n",
       "      <td>force</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pleased</td>\n",
       "      <td>colleague</td>\n",
       "      <td>security</td>\n",
       "      <td>speak</td>\n",
       "      <td>american</td>\n",
       "      <td>compromise</td>\n",
       "      <td>central</td>\n",
       "      <td>distinguish</td>\n",
       "      <td>increase</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>urge</td>\n",
       "      <td>time</td>\n",
       "      <td>united</td>\n",
       "      <td>north</td>\n",
       "      <td>defense</td>\n",
       "      <td>staff</td>\n",
       "      <td>international</td>\n",
       "      <td>unanimous</td>\n",
       "      <td>country</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     president      senator        nato         time     united  \\\n",
       "0        madam       minute    european       friend     nation   \n",
       "1        happy        floor      europe  distinguish      vital   \n",
       "2       senior    remainder      russia    colleague   military   \n",
       "3        speak        thank   stability     carolina    protect   \n",
       "4  accommodate        yield    military        chair  stability   \n",
       "5         vote  distinguish      poland        thank   national   \n",
       "6    hampshire    delighted  membership    unanimous   economic   \n",
       "7      consent       friend     central    amendment   increase   \n",
       "8      pleased    colleague    security        speak   american   \n",
       "9         urge         time      united        north    defense   \n",
       "\n",
       "            bill        country    amendment     nation      senate  \n",
       "0      introduce          force        speak     united     pleased  \n",
       "1     republican         russia    colleague  stability      debate  \n",
       "2          offer          power         vote   military        urge  \n",
       "3          issue       military         time      vital  compliment  \n",
       "4    legislation      stability     carolina    protect      senior  \n",
       "5  appropriation         threat    president   economic        vote  \n",
       "6           urge         nation       friend     russia    majority  \n",
       "7           pass          enter        chair      force   president  \n",
       "8     compromise        central  distinguish   increase     comment  \n",
       "9          staff  international    unanimous    country   introduce  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4c175-648c-4bec-ac60-b865bed8432c",
   "metadata": {},
   "source": [
    "Here I constructed a dataframe which provides top 10 similar words for each word in the list of top10words.\n",
    "\n",
    "For example, most similar words to nato were european, membership, stability, poland, etc.\n",
    "\n",
    "Let's see whether top 10 bigrams are in the list of top words and their similars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62261c0b-59e2-4a02-8f14-294368fabc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united nation',\n",
       " 'foreign policy',\n",
       " 'chemical weapon',\n",
       " 'nato enlargement',\n",
       " 'foreign relation',\n",
       " 'yield floor',\n",
       " 'nuclear weapon',\n",
       " 'relation committee',\n",
       " 'north carolina',\n",
       " 'madam president']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0292b12-6200-47fa-afa3-3898c4bb7749",
   "metadata": {},
   "source": [
    "We see that both united and nation are in the list of top words. Moreover, top similar word to united is nation, whereas top 2 similar word to nation is united. \n",
    "\n",
    "Foreign policy and chemical weapon are not in the top words list.\n",
    "\n",
    "Nato is the third top word. Top 5 similar word to nato is enlarge, which is a verb version of enlargement.\n",
    "\n",
    "The last bigram which is present in the df_similar is madam president. President is the most common word in the text. Second most similar word to president is madam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2e052-9e03-4209-95ff-2bd12d7ec338",
   "metadata": {},
   "source": [
    "# Challenge 2: Use the senator speeches in the folder 105-extracted-date and use cosine similarity to find whose senator speech is closest to senator Biden. Use sen105kh_fix.csv and/or Wikipedia to validate your findings (i.e., understand if the most similar speeches are senators from the same state and/party). Describe your findings. (Compare with the outcome you got using cosine similarity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d935aad6-42bf-4c2e-b0fa-fa6db27bdc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 files in the folder\n"
     ]
    }
   ],
   "source": [
    "# Accessing files in the necessary folder\n",
    "files = os.listdir('../Inputs/105-extracted-date')\n",
    "print(\"There are {:} files in the folder\".format(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de26f5a-c0c5-425a-96c4-755435464178",
   "metadata": {},
   "source": [
    "Create a list of documents. Text from each of 100 documents will be appended to the list *list_docs* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5ffaae7-7a56-4e50-915b-6b65c96dd688",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_docs=[]\n",
    "\n",
    "for file in files:\n",
    "    xml = open(os.path.join('../Inputs/105-extracted-date', file)).read() \n",
    "    soup = BeautifulSoup(xml) #use BeautifulSoup library for parsing contents since documents have XML format\n",
    "    doc = ''\n",
    "    for word in soup.find_all('text'): #loop for extracting only text within <text> tag\n",
    "        find_all_syntax=word.get_text()\n",
    "        doc = doc + find_all_syntax\n",
    "    list_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e481331-f727-4a42-9a5f-27a20694344f",
   "metadata": {},
   "source": [
    "### Load the preprocessed data (the pickle we created for the group assignment)\n",
    "Below is the function that allows to load the pickle. Since preprocessing takes some time, we preprocessed the data one time and saved it as pickle for saving time purposes. You can either choose to open the pickle provided or run the code for pre-processing, which would take some time (default option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8d890ef-ac94-46f8-a9e9-4fd3687f9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"prepocessed_docs.pkl\", \"rb\") as fp:   # Unpickling\n",
    "    speech_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae737e8-b5be-4b79-9c85-dc310dc14088",
   "metadata": {},
   "source": [
    "To preprocess text without using pickle (takes some time).\n",
    "\n",
    "Here we transform the original list of documents to the list of preprocessed speech documents named *speech_list*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a45f3f5c-77e8-48fb-9945-4b8d3487106c",
   "metadata": {},
   "source": [
    "speech_list = []\n",
    "for doc in list_docs:\n",
    "    speech_list.append(preprocessing_text(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814b1d6-9f70-4c4e-aa88-9c3b15203177",
   "metadata": {},
   "source": [
    "## TF-IDF Cosine Similarity (as was done previously by the group)\n",
    "\n",
    "After we preprocessed data, we want to transform the collection of preprocessed text to a matrix of TF-IDF features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c21023-f49f-4068-b7cc-31ccf625e76f",
   "metadata": {},
   "source": [
    "We need to choose right parameters. \n",
    "\n",
    "Our team thinks that *max_df* should be 85, that is we want vectorizer to ignore terms that appear in more than 85 documents (85%). The words like \"president\", \"senator\", or \"budget\" would appear almost in every speech, so we want to exclude them from the analysis.\n",
    "\n",
    "Moreover, we think that *min_df* should be 2, that is we want vectorizer to ignore terms that appear in less than 2 documents (2%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f7985fa-f3a3-4ee8-8cc9-f3f0dcdb78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 2, max_df = 85) \n",
    "doc_vector = vectorizer.fit_transform(speech_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3568f-3644-450e-a003-d5ab5831cfeb",
   "metadata": {},
   "source": [
    "Convert the vectors to dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a842b8b5-b018-4ea6-8507-5a22195741b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.DataFrame(doc_vector.toarray().transpose(), index=vectorizer.get_feature_names())\n",
    "\n",
    "#rename columns name\n",
    "for i in range(100):\n",
    "    df = df.rename(columns={i: files[i][4:-4]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264413c-f0d5-4890-a6e2-f2ab79edc601",
   "metadata": {},
   "source": [
    "Here we calculate cosine similarity of each speech with Biden's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "292d987e-8ff6-4fcc-ba27-047f82aa5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce603501-c8db-401e-8352-676d57f6f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to array\n",
    "corpus_array = df.to_numpy()\n",
    "\n",
    "similarity_biden = [] #list of similarities values for Biden's speech\n",
    "\n",
    "# compare each speech with Biden's\n",
    "for i in range(100):\n",
    "    temp_sim = cosine_similarity(corpus_array[:,i].reshape(1, -1), corpus_array[:,6].reshape(1, -1)) # 6th column belongs to Biden's speech\n",
    "    similarity_biden.append(temp_sim[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8c641-3721-4eeb-9094-e818333ad0ca",
   "metadata": {},
   "source": [
    "Let's look on speech similarities. We can see that the Biden's speech got cosine similarity of 1 with itself, thus the outlier on the graph. Most of the documents have cosine similarity somewhere between 0 and 0.4.\n",
    "\n",
    "The values could be higher if we set *max_df* to 100, but that is not necessary as we discussed this above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11e7267b-4483-46cf-9021-a8486cdf01af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of similarities values\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(similarity_biden, bins=30, range=[0.0,1])\n",
    "plt.title(\"Cosine similarity histogram\")\n",
    "plt.xlabel(\"Cosine similarity\")\n",
    "plt.ylabel(\"# of documents\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bb7b8-2df7-49cc-8bb6-4f4ecaf033a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation of findings (i.e., understand if the most similar speeches are senators from the same state and/party)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e027a7fe-cbb8-4db7-9290-95d92ccedf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read given csv with info about senators\n",
    "senators_df = pd.read_csv('../Inputs/sen105kh_fix.csv',sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57f215-b536-48db-a862-bd7b6251c29b",
   "metadata": {},
   "source": [
    "Now we want to add a column with similarities to the *senators_df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61eb1123-4f11-45ae-99e8-c8f573367727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column in a format \"lname-stateab\"\n",
    "senators_df['lname_state'] = senators_df[['lname', 'stateab']].agg('-'.join, axis=1)\n",
    "senators_df\n",
    "\n",
    "# create a df with names and cosine similarities\n",
    "similarity_df = pd.DataFrame(\n",
    "    {'lname_state': list(df.columns.values), # list of names-states\n",
    "     'tfidf_similarity': similarity_biden\n",
    "    })\n",
    "\n",
    "# join two dataframes\n",
    "senators_df = pd.merge(senators_df,similarity_df,on='lname_state',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc70639-5f82-4dfc-adc0-2217d9a27add",
   "metadata": {},
   "source": [
    "The next step is to clean the dataframe. There is only one change we should do: replace numbers in party with the correct name. As Biden is Democrat and his party is 100, we replace 100 with 'democratic'. \n",
    "\n",
    "Also add a dummy *is_democrat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89b71868-4b2f-429c-ab55-ca250c92c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "senators_df['party'] = senators_df['party'].replace(100, 'democratic')\n",
    "senators_df['party'] = senators_df['party'].replace(200, 'republican')\n",
    "senators_df['is_democrat'] = np.where(senators_df['party'] == 'democratic', 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89f8ae-d015-4904-ae5a-be6e745820b6",
   "metadata": {},
   "source": [
    "Sort df by cosine similarity value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1d9013e-18f9-4eb4-a73b-36728950fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "senators_df= senators_df.sort_values(by=['tfidf_similarity'], ascending=False)\n",
    "senators_df['tdidf_rank'] = senators_df['tfidf_similarity'].rank(ascending=False).astype(int)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee54d25-46ea-4a94-83b6-356a635f3509",
   "metadata": {},
   "source": [
    "Now let's look on the info about top 10 speeches similar to Biden's one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bca004e9-509e-494f-a4bf-da27ea9f6184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cong</th>\n",
       "      <th>lname</th>\n",
       "      <th>stateab</th>\n",
       "      <th>lstate</th>\n",
       "      <th>id</th>\n",
       "      <th>dist</th>\n",
       "      <th>party</th>\n",
       "      <th>lname_state</th>\n",
       "      <th>tfidf_similarity</th>\n",
       "      <th>is_democrat</th>\n",
       "      <th>tdidf_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>105</td>\n",
       "      <td>biden</td>\n",
       "      <td>de</td>\n",
       "      <td>DELAWAR</td>\n",
       "      <td>14101</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>biden-de</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>105</td>\n",
       "      <td>roberts</td>\n",
       "      <td>ks</td>\n",
       "      <td>KANSAS</td>\n",
       "      <td>14852</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>roberts-ks</td>\n",
       "      <td>0.337045</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>105</td>\n",
       "      <td>roth</td>\n",
       "      <td>de</td>\n",
       "      <td>DELAWAR</td>\n",
       "      <td>11044</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>roth-de</td>\n",
       "      <td>0.280060</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>105</td>\n",
       "      <td>warner</td>\n",
       "      <td>va</td>\n",
       "      <td>VIRGINI</td>\n",
       "      <td>14712</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>warner-va</td>\n",
       "      <td>0.276743</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>105</td>\n",
       "      <td>lieberman</td>\n",
       "      <td>ct</td>\n",
       "      <td>CONNECT</td>\n",
       "      <td>15704</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>lieberman-ct</td>\n",
       "      <td>0.269329</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>kyl</td>\n",
       "      <td>az</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>15429</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>kyl-az</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>105</td>\n",
       "      <td>smith</td>\n",
       "      <td>or</td>\n",
       "      <td>OREGON</td>\n",
       "      <td>49705</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>smith-or</td>\n",
       "      <td>0.263810</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>105</td>\n",
       "      <td>byrd</td>\n",
       "      <td>wv</td>\n",
       "      <td>WEST VI</td>\n",
       "      <td>1366</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>byrd-wv</td>\n",
       "      <td>0.256887</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>105</td>\n",
       "      <td>kerry</td>\n",
       "      <td>ma</td>\n",
       "      <td>MASSACH</td>\n",
       "      <td>14920</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>kerry-ma</td>\n",
       "      <td>0.249483</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>105</td>\n",
       "      <td>damato</td>\n",
       "      <td>ny</td>\n",
       "      <td>NEW YOR</td>\n",
       "      <td>14900</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>damato-ny</td>\n",
       "      <td>0.246512</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>105</td>\n",
       "      <td>dodd</td>\n",
       "      <td>ct</td>\n",
       "      <td>CONNECT</td>\n",
       "      <td>14213</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>dodd-ct</td>\n",
       "      <td>0.234788</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cong      lname stateab   lstate     id  dist       party   lname_state  \\\n",
       "14   105      biden      de  DELAWAR  14101     0  democratic      biden-de   \n",
       "31   105    roberts      ks  KANSAS   14852     0  republican    roberts-ks   \n",
       "15   105       roth      de  DELAWAR  11044     0  republican       roth-de   \n",
       "91   105     warner      va  VIRGINI  14712     0  republican     warner-va   \n",
       "13   105  lieberman      ct  CONNECT  15704     0  democratic  lieberman-ct   \n",
       "4    105        kyl      az  ARIZONA  15429     0  republican        kyl-az   \n",
       "72   105      smith      or  OREGON   49705     0  republican      smith-or   \n",
       "94   105       byrd      wv  WEST VI   1366     0  democratic       byrd-wv   \n",
       "41   105      kerry      ma  MASSACH  14920     0  democratic      kerry-ma   \n",
       "62   105     damato      ny  NEW YOR  14900     0  republican     damato-ny   \n",
       "12   105       dodd      ct  CONNECT  14213     0  democratic       dodd-ct   \n",
       "\n",
       "    tfidf_similarity  is_democrat  tdidf_rank  \n",
       "14          1.000000            1           0  \n",
       "31          0.337045            0           1  \n",
       "15          0.280060            0           2  \n",
       "91          0.276743            0           3  \n",
       "13          0.269329            1           4  \n",
       "4           0.263852            0           5  \n",
       "72          0.263810            0           6  \n",
       "94          0.256887            1           7  \n",
       "41          0.249483            1           8  \n",
       "62          0.246512            0           9  \n",
       "12          0.234788            1          10  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senators_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f45ed-31ae-4b52-aff6-21556afae8c7",
   "metadata": {},
   "source": [
    "As we can see, top 3 similar speeches were given by republicans. This is a bit surprising, since we expected to see more democrats on the top. However, at the same time, politically Biden was known as a centrist who would work “across-the-aisle” with his Republican counterparts and so while it may be initially surprising that he doesn't align with members of his own party, he does align well with centrist members of both parties. \n",
    "\n",
    "In addition, we can see that the second similar speech to Biden's was given by Roth, who is from the same state. It is reasonable that senators from the same state would talk about similar things.\n",
    "\n",
    "## Doc2Vec cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8202a28-716a-4081-89bf-53132d30b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=40, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e30318f-f2f7-4267-ba29-c047baa20425",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_split = [split(speech) for speech in speech_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "861c20d7-85b1-4669-8f48-65ff3e93610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(len(speech_split)):\n",
    "    corpus.append(gensim.models.doc2vec.TaggedDocument(words=speech_split[i], tags=[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c18e5b0b-d505-40bb-ba06-7dd2c5f29c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66b61d6f-8dd7-43b9-8944-8116e3402b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82dd350a-1ebc-4dd7-b7e6-7f4596610010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biden's speech is No.6\n",
    "biden_vector = model.infer_vector(corpus[6].words)\n",
    "similarity_biden = model.dv.most_similar([biden_vector], topn=len(model.dv))\n",
    "\n",
    "similarity_rank = [doc[0] for doc in similarity_biden]\n",
    "similarity_values = [doc[1] for doc in similarity_biden]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2d7dd88-1a5e-45b4-87c7-43018ab230df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list in a format \"lname-stateab\" following the order from similarity_rank\n",
    "lname_state = []\n",
    "\n",
    "for i in similarity_rank:\n",
    "    name = files[i][4:-4]\n",
    "    lname_state.append(name)\n",
    "\n",
    "# creating a dataframe with doc2vec values\n",
    "doc2vecdf = pd.DataFrame(list(zip(lname_state, similarity_values)),\n",
    "               columns =['lname_state', 'doc2vec_similarity']) \n",
    "\n",
    "# creating a rank for doc2vec values\n",
    "doc2vecdf['doc2vec_rank'] = doc2vecdf['doc2vec_similarity'].rank(ascending=False).astype(int)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0adf0a54-34d4-4092-8a7d-75dd44034db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending doc2vecdf to the senators df\n",
    "senators_df = pd.merge(senators_df,doc2vecdf,on='lname_state',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34d29f81-ce26-4bb6-b4c6-e6de3d0dd755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cong</th>\n",
       "      <th>lname</th>\n",
       "      <th>stateab</th>\n",
       "      <th>lstate</th>\n",
       "      <th>id</th>\n",
       "      <th>dist</th>\n",
       "      <th>party</th>\n",
       "      <th>lname_state</th>\n",
       "      <th>tfidf_similarity</th>\n",
       "      <th>is_democrat</th>\n",
       "      <th>tdidf_rank</th>\n",
       "      <th>doc2vec_similarity</th>\n",
       "      <th>doc2vec_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105</td>\n",
       "      <td>biden</td>\n",
       "      <td>de</td>\n",
       "      <td>DELAWAR</td>\n",
       "      <td>14101</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>biden-de</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>105</td>\n",
       "      <td>kerrey</td>\n",
       "      <td>ne</td>\n",
       "      <td>NEBRASK</td>\n",
       "      <td>15702</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>kerrey-ne</td>\n",
       "      <td>0.204765</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.398721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>105</td>\n",
       "      <td>moynihan</td>\n",
       "      <td>ny</td>\n",
       "      <td>NEW YOR</td>\n",
       "      <td>14508</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>moynihan-ny</td>\n",
       "      <td>0.188003</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.394715</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>105</td>\n",
       "      <td>byrd</td>\n",
       "      <td>wv</td>\n",
       "      <td>WEST VI</td>\n",
       "      <td>1366</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>byrd-wv</td>\n",
       "      <td>0.256887</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.383699</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>105</td>\n",
       "      <td>lugar</td>\n",
       "      <td>in</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>14506</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>lugar-in</td>\n",
       "      <td>0.196743</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.375562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>105</td>\n",
       "      <td>helms</td>\n",
       "      <td>nc</td>\n",
       "      <td>NORTH C</td>\n",
       "      <td>14105</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>helms-nc</td>\n",
       "      <td>0.135327</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.351517</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>105</td>\n",
       "      <td>specter</td>\n",
       "      <td>pa</td>\n",
       "      <td>PENNSYL</td>\n",
       "      <td>14910</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>specter-pa</td>\n",
       "      <td>0.161148</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.351434</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>105</td>\n",
       "      <td>kyl</td>\n",
       "      <td>az</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>15429</td>\n",
       "      <td>0</td>\n",
       "      <td>republican</td>\n",
       "      <td>kyl-az</td>\n",
       "      <td>0.263852</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.350025</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>105</td>\n",
       "      <td>bryan</td>\n",
       "      <td>nv</td>\n",
       "      <td>NEVADA</td>\n",
       "      <td>15700</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>bryan-nv</td>\n",
       "      <td>0.078077</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>0.343953</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>105</td>\n",
       "      <td>kohl</td>\n",
       "      <td>wi</td>\n",
       "      <td>WISCONS</td>\n",
       "      <td>15703</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>kohl-wi</td>\n",
       "      <td>0.150045</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.338671</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>105</td>\n",
       "      <td>lautenberg</td>\n",
       "      <td>nj</td>\n",
       "      <td>NEW JER</td>\n",
       "      <td>14914</td>\n",
       "      <td>0</td>\n",
       "      <td>democratic</td>\n",
       "      <td>lautenberg-nj</td>\n",
       "      <td>0.163525</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.338388</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cong       lname stateab   lstate     id  dist       party    lname_state  \\\n",
       "0    105       biden      de  DELAWAR  14101     0  democratic       biden-de   \n",
       "16   105      kerrey      ne  NEBRASK  15702     0  democratic      kerrey-ne   \n",
       "27   105    moynihan      ny  NEW YOR  14508     0  democratic    moynihan-ny   \n",
       "7    105        byrd      wv  WEST VI   1366     0  democratic        byrd-wv   \n",
       "21   105       lugar      in  INDIANA  14506     0  republican       lugar-in   \n",
       "58   105       helms      nc  NORTH C  14105     0  republican       helms-nc   \n",
       "36   105     specter      pa  PENNSYL  14910     0  republican     specter-pa   \n",
       "5    105         kyl      az  ARIZONA  15429     0  republican         kyl-az   \n",
       "99   105       bryan      nv  NEVADA   15700     0  democratic       bryan-nv   \n",
       "45   105        kohl      wi  WISCONS  15703     0  democratic        kohl-wi   \n",
       "35   105  lautenberg      nj  NEW JER  14914     0  democratic  lautenberg-nj   \n",
       "\n",
       "    tfidf_similarity  is_democrat  tdidf_rank  doc2vec_similarity  \\\n",
       "0           1.000000            1           0            0.998079   \n",
       "16          0.204765            1          16            0.398721   \n",
       "27          0.188003            1          27            0.394715   \n",
       "7           0.256887            1           7            0.383699   \n",
       "21          0.196743            0          21            0.375562   \n",
       "58          0.135327            0          58            0.351517   \n",
       "36          0.161148            0          36            0.351434   \n",
       "5           0.263852            0           5            0.350025   \n",
       "99          0.078077            1          99            0.343953   \n",
       "45          0.150045            1          45            0.338671   \n",
       "35          0.163525            1          35            0.338388   \n",
       "\n",
       "    doc2vec_rank  \n",
       "0              0  \n",
       "16             1  \n",
       "27             2  \n",
       "7              3  \n",
       "21             4  \n",
       "58             5  \n",
       "36             6  \n",
       "5              7  \n",
       "99             8  \n",
       "45             9  \n",
       "35            10  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senators_df.sort_values('doc2vec_rank').head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f00f8-14d5-42b4-b193-c6e4d5dbfdd8",
   "metadata": {},
   "source": [
    "As we can see, the doc2vec ranking is different from the tf-idf ranking. Only two of top 10 similar documents are present in the list of top 10 similar by doc2vec. Moreover, Bryan's speech, which was ranked 99 (pre least similar by tf-idf), is not is 8th most similar according to doc2vec.\n",
    "\n",
    "However, top-3 similar speeches by doc2vec are given by democrats. New York (2), West Virginia (3), Indiana (4), North Carolina (5), Pennsylvania (6) are located on the East coast near the Delaware. Probably that's the reason the speeches look similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e525c6e-b1c3-478b-a907-05c80816d907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "allDone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
